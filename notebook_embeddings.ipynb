{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ca9f4775",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model... this may take a minute.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at FacebookAI/roberta-large-mnli were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Label Map: {0: 'CONTRADICTION', 1: 'NEUTRAL', 2: 'ENTAILMENT'}\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Define the model checkpoint\n",
    "model_name = \"FacebookAI/roberta-large-mnli\"\n",
    "\n",
    "print(\"Loading model... this may take a minute.\")\n",
    "\n",
    "# 1. Load Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# 2. Load Model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "# 3. Verify Label Mapping (Crucial Step)\n",
    "# We expect: 0 -> Contradiction (NTA), 2 -> Entailment (YTA)\n",
    "print(f\"Model Label Map: {model.config.id2label}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4ef2c8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\brzak\\AppData\\Local\\Temp\\ipykernel_5696\\2733097480.py:1: DtypeWarning: Columns (0,2,4,11,20,22,33,35,36,41,80,81,82,84,85,89,92,95,96,100,101,113,114,121,123,130,134,135,136,144,147,155,162,164,168,169,172,173,174,175,176) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.read_csv(\"data/samples/sample_20000.csv\")\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"data/samples/sample_20000.csv\")\n",
    "\n",
    "data = data[[\"title\", \"selftext\", \"link_flair_text\"]]\n",
    "\n",
    "asshole_flairs = [\"asshole\", \n",
    "                  \"slight asshole\",\n",
    "                  \"Asshole\", \n",
    "                  \"asshole (a bit)\", \n",
    "                  \"Obvious Asshole\",\n",
    "                  \"Asshole (but funny/justified)\", \n",
    "                  \"justified asshole\",\n",
    "                  \"huge asshole\", \n",
    "                  \"asshole (Kind of)\",\n",
    "                  \"asshole (tiny bit)\", \n",
    "                  \"Crouching Liar; hidden asshole\",\n",
    "                  \"Not the A-hole POO Mode\",\n",
    "                  \"Asshole POO Mode\",\n",
    "                  \"asshole\"]\n",
    "\n",
    "not_enough_info_flairs = [\"not enough info\",\n",
    "                          \"no assholes here\",\n",
    "                          \"ambiguous\"]\n",
    "\n",
    "not_an_asshole_flairs = [\"not the asshole\",\n",
    "                         \"not the a-hole\",\n",
    "                         \"Not the A-hole\",\n",
    "                         \"Not the A-hole POO Mode\",\n",
    "                         \"justified\"]\n",
    "\n",
    "def assign_target(flair):\n",
    "    if flair in asshole_flairs:\n",
    "        return 1\n",
    "    elif flair in not_enough_info_flairs:\n",
    "        return 2\n",
    "    elif flair in not_an_asshole_flairs:\n",
    "        return 0\n",
    "    else:\n",
    "        raise ValueError(\"Unexpected flair: {}\".format(flair))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8509908",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign target labels\n",
    "data[\"target\"] = data[\"link_flair_text\"].apply(assign_target)\n",
    "\n",
    "# Remove ambiguous cases\n",
    "data = data[data[\"target\"]!=2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c9b4d161",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((15834, 2), (160, 2), (3999, 2))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data[[\"title\", \"selftext\"]],\n",
    "    data[\"target\"],\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=data[\"target\"]\n",
    ")\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    test_size=160,\n",
    "    random_state=42,\n",
    "    stratify=y_train\n",
    ")\n",
    "\n",
    "X_train.shape, X_val.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae6d115",
   "metadata": {},
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5bf64ce0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores: [0.5896820425987244, 0.3716268837451935, 0.038691096007823944]\n",
      "Winner: asshole\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'sequence': 'I broke up with my boyfriend because he screamed at me every day.',\n",
       " 'labels': ['asshole', 'ambiguous', 'justified'],\n",
       " 'scores': [0.5896820425987244, 0.3716268837451935, 0.038691096007823944]}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# 2. DEFINE YOUR LABELS\n",
    "# The pipeline will auto-generate hypotheses like \"This text is about {label}.\"\n",
    "candidate_labels = [\"asshole\", \"justified\", \"ambiguous\"]\n",
    "\n",
    "# 3. TEST IT\n",
    "post = \"I broke up with my boyfriend because he screamed at me every day.\"\n",
    "\n",
    "result = classifier(post, candidate_labels)\n",
    "\n",
    "print(f\"Scores: {result['scores']}\")\n",
    "print(f\"Winner: {result['labels'][0]}\")\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "474f10e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_verdict_forced(post):\n",
    "    hypothesis = \"The author of this story is being an asshole.\"\n",
    "    \n",
    "    # 1. Encode\n",
    "    input_ids = tokenizer.encode(\n",
    "        post, \n",
    "        hypothesis, \n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        max_length=512\n",
    "    )\n",
    "    \n",
    "    # 2. Get Logits\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids)\n",
    "        logits = outputs.logits[0]  # Shape: [3] -> [Contradiction, Neutral, Entailment]\n",
    "\n",
    "    # --- THE HEAVY LIFTING FIX ---\n",
    "    # RoBERTa MNLI mapping: 0=Contradiction (NTA), 1=Neutral, 2=Entailment (YTA)\n",
    "    \n",
    "    # We slice out only indices 0 and 2\n",
    "    binary_logits = torch.tensor([logits[0], logits[2]]) \n",
    "    \n",
    "    # We re-calculate softmax on just these two options\n",
    "    probs = F.softmax(binary_logits, dim=0)\n",
    "    \n",
    "    nta_prob = probs[0].item() # Contradiction\n",
    "    yta_prob = probs[1].item() # Entailment (now index 1 of our binary tensor)\n",
    "\n",
    "    # 3. Simple Decision\n",
    "    if yta_prob > nta_prob:\n",
    "        return 1 # YTA\n",
    "    else:\n",
    "        return 0 # NTA\n",
    "\n",
    "# --- Run your loop again with this function ---\n",
    "\n",
    "X_val['prediction'] = X_val['selftext'].apply(predict_verdict_forced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e088c19b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.79      0.78       126\n",
      "           1       0.18      0.18      0.18        34\n",
      "\n",
      "    accuracy                           0.66       160\n",
      "   macro avg       0.48      0.48      0.48       160\n",
      "weighted avg       0.65      0.66      0.65       160\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_val, X_val[\"prediction\"], zero_division=0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
